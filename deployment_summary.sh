#!/bin/bash
# Deployment Summary and Quick Commands

echo "=================================================="
echo "üöÄ CEPHALOMETRIC MODEL DEPLOYMENT SUMMARY"
echo "=================================================="
echo ""
echo "üìÅ Created Files:"
echo "  1. prepare_model_for_deployment.py - Extracts and publishes model"
echo "  2. convert_to_onnx.py - Converts to ONNX format"
echo "  3. inference_cpu.py - CPU inference script"
echo "  4. DEPLOYMENT_GUIDE.md - Detailed instructions"
echo ""
echo "=================================================="
echo "‚ö° QUICK START COMMANDS"
echo "=================================================="
echo ""
echo "On GPU Machine (where you trained):"
echo "-----------------------------------"
echo ""
echo "# Step 1: Prepare deployment package"
echo "python prepare_model_for_deployment.py --model_idx 2 --epoch 99"
echo ""
echo "# Step 2: Convert to ONNX"
echo "python convert_to_onnx.py"
echo ""
echo "# Optional: Create tarball for transfer"
echo "tar -czf deployment_models.tar.gz deployment_package/ onnx_models/"
echo ""
echo "=================================================="
echo ""
echo "On CPU Server:"
echo "--------------"
echo ""
echo "# Step 1: Install dependencies"
echo "pip install onnxruntime opencv-python numpy matplotlib"
echo ""
echo "# Step 2: Extract models (if using tarball)"
echo "tar -xzf deployment_models.tar.gz"
echo ""
echo "# Step 3: Run inference"
echo "python inference_cpu.py --image test.jpg --visualize"
echo ""
echo "# For faster inference (2x speedup)"
echo "cd onnx_models && python optimize_for_cpu.py && cd .."
echo "python inference_cpu.py --use_quantized --image test.jpg"
echo ""
echo "=================================================="
echo "üìã DEPLOYMENT CHECKLIST"
echo "=================================================="
echo ""
echo "‚úÖ 1. Run prepare_model_for_deployment.py on GPU machine"
echo "‚úÖ 2. Run convert_to_onnx.py on GPU machine"
echo "‚úÖ 3. Transfer onnx_models/ folder to CPU server"
echo "‚úÖ 4. Transfer inference_cpu.py to CPU server"
echo "‚úÖ 5. Install dependencies on CPU server"
echo "‚úÖ 6. Test with single image first"
echo "‚úÖ 7. Optional: Quantize models for 2x speedup"
echo ""
echo "=================================================="
echo "üìä EXPECTED PERFORMANCE"
echo "=================================================="
echo ""
echo "Model Size:"
echo "  - Original HRNet: ~35MB ‚Üí ~17MB (published)"
echo "  - ONNX HRNet: ~17MB ‚Üí ~4MB (quantized)"
echo "  - MLP: ~1MB ‚Üí ~250KB (quantized)"
echo ""
echo "Inference Speed (Intel i7 CPU):"
echo "  - Original: ~50-100ms per image"
echo "  - Quantized: ~25-50ms per image"
echo "  - Batch processing: Better throughput"
echo ""
echo "=================================================="
echo "üí° TIPS"
echo "=================================================="
echo ""
echo "1. Use --use_quantized for production (2x faster)"
echo "2. Process multiple images with --image_list"
echo "3. Save results with --save_json flag"
echo "4. Visualize predictions with --visualize"
echo "5. Check DEPLOYMENT_GUIDE.md for API integration examples"
echo ""
echo "==================================================" 